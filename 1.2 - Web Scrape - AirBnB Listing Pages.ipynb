{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef91ecd",
   "metadata": {},
   "source": [
    "# Webscrape Individual AirBnB Listing Pages\n",
    "\n",
    "In Part 1 of the webscraping, we scraped 180,000~ AirBnB listings in the western half of the United States. In the process of that webscraping, we got each listing's `listing_id` -- their unique ID in AirBnB's system.\n",
    "\n",
    "Using this as a stepping stone, we can programmatically step from the search page into each listing page, and access a much richer/wider range of data than what exists on the search page.\n",
    "\n",
    "This notebook is going to scrape individual listing pages to put together a larger picture of the AirBnB listings.\n",
    "\n",
    "_An example of a listing page._\n",
    "\n",
    "![Image of individual listing](https://lh5.googleusercontent.com/5Oxx59oyuTQE_oEHvRvs_kNykfOsv1e1xsrpJFRJ4cnN0ve4xEGpvhb4BpHrTNykBDM=w2400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb5e65",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eaabb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webscraping.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from requests_html import HTMLSession\n",
    "import json\n",
    "\n",
    "# For standard data manipulation.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# For progress tracking.\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Makes it easier to see all the columns in wide dataframes!\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db815558",
   "metadata": {},
   "source": [
    "# Read in webscraped data from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fbaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scraped_listings BACKUP.csv')\n",
    "df.drop(columns=['Unnamed: 0','Unnamed: 0.8','Unnamed: 0.9'],inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659450e",
   "metadata": {},
   "source": [
    "# Set Headers\n",
    "\n",
    "We want to make our requests to AirBnB look as human as possible to prevent getting blocked. VPN's can always help if they block an IP address, but we'd prefer to not get blocked in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cookie': 'OptanonAlertBoxClosed=NR; OptanonAlertBoxClosed=NR; bev=1671467614_NDg3YWI1OWViODE0; country=US; everest_cookie=1671467934.o8UVAqq76D3QPRXxo5SV.2TXJ26qIr6wRp2aGPSqpNu0hmcaL7JhFzmDBWbnzmDw; _csrf_token=V4%24.airbnb.com%24Dwa4m1DnRds%24l_ThNsfxVtYzJnV3gf7vehcjPJusOG_QBcYxaP2lZOM%3D; flags=0; _gcl_au=1.1.1979572412.1671467936; _ga=GA1.1.923589191.1671467936; tzo=-420; frmfctr=wide; ak_bmsc=2BF62A04339B5A8040F6BE42D7397A3E~000000000000000000000000000000~YAAQjuXEF4UQhgeFAQAA27JjLRJCqxbQs87wSwEdHRYX0G6/TDKZTovu4o6yCZ81IqZ6MhWckkFsgOG1eRvCyGcwYcGwZT6Hj0lKPA7+LECJW/p7uLVBPkdj9vDMc+M3APPxtp3dA4N7Xiz9k2CsK1cScWB15o99WkNT8TblkPRJMP+//tibMWG2xoEjeEV14znNbpxu/a19eOVXAFS6pvZFEKc1PvKaN6qVTNMeaHczy8vtIlUIhOHm8n0/neIB5WjhQewZ2bsI4Jk5FAyuugtHiXFiRJU7poWmTC1z3vQV5BNZly4umCoBfxL+4/1Gwtvg+5jzWIgEsQIOlNvhj6IhTWNdgIiUubyhm6SwXUHdD2r80FCzSqqkgs/F95M/C2eTuiMmpV414Qo=; jitney_client_session_id=d94ad55f-160c-4b75-aa19-6b94073c92fc; jitney_client_session_created_at=1671505980; _user_attributes=%7B%22curr%22%3A%22USD%22%2C%22guest_exchange%22%3A1.0%2C%22device_profiling_session_id%22%3A%221671467934--953d4ec0c3e885ffcfcecdcb%22%2C%22giftcard_profiling_session_id%22%3A%221671503786--bedf36f5fd7bc1727463feba%22%2C%22reservation_profiling_session_id%22%3A%221671503786--104feb525ef2701da84b3359%22%7D; jitney_client_session_updated_at=1671505982; _ga_2P6Q8PGG16=GS1.1.1671505983.4.1.1671505983.0.0.0; _uetsid=a14c3a907fbb11ed80888db3ffb5b814; _uetvid=a14c54c07fbb11edbfa35d16156e96a6; previousTab=%7B%22id%22%3A%22c5e39694-2720-411d-bbb4-5880ad1455b6%22%2C%22url%22%3A%22https%3A%2F%2Fwww.airbnb.com%2Fs%2FUnited-States%2Fhomes%22%7D; bm_sv=67E8D97D583DC2BA73262B2A33B8BD6A~YAAQjuXEF3gehweFAQAAzlqFLRKkxd6iaP1xq0hHbfO3CsyIIQifNN1kg5O8tYdobGiFUFkAU0ek5Tg67B28nu2btfk+1yHhT6TE1jibTRrmUjxmCnS6gZ9Sclthu91yIEEePEd32A66YE2G6ofZjSQCTS8TsZU9O8pKFQ7Nid5vVQ6DlyEPLX5yEYMLK94A1kBNulzVg+KjaWf2UN8pSkr66hIUxNn5Qq3fQtl8kCxJeSb6MGW977OC++DGDRXKqg==~1; cfrmfctr=MOBILE; cbkp=2',\n",
    "    'device-memory': '8',\n",
    "    'dpr': '2',\n",
    "    'ect': '4g',\n",
    "    'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\", \"Google Chrome\";v=\"108\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'empty',\n",
    "    'sec-fetch-mode': 'no-cors',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'viewport-width': '895'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c703fba",
   "metadata": {},
   "source": [
    "# Create the helper functions required to run the scraper\n",
    "\n",
    "Let's set up a few functions we'll use to breathe life into this webscraper.\n",
    "\n",
    "\n",
    "### def `reset_dataframe`:\n",
    "**Args:** \n",
    "* None.\n",
    "\n",
    "**Returns:**\n",
    "* Empty DataFrame with the shape and headers of the data we intend to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8830d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_dataframe():\n",
    "\n",
    "    # Create the dataframe.\n",
    "    pages_dataframe = pd.DataFrame(\n",
    "            {'listing_id':           [],\n",
    "             'n_guests':             [],\n",
    "             'n_bedrooms':           [],\n",
    "             'n_beds':               [],\n",
    "             'n_baths':              [],\n",
    "             'n_amenities':          [],\n",
    "             'amenities':            [],\n",
    "             'rating_cleanliness':   [],\n",
    "             'rating_communication': [],\n",
    "             'rating_checkin':       [],\n",
    "             'rating_accuracy':      [],\n",
    "             'rating_location':      [],\n",
    "             'rating_value':         [],\n",
    "             'sleeping':             [],\n",
    "             'description':          []\n",
    "            })\n",
    "    \n",
    "    return pages_dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9d4d2",
   "metadata": {},
   "source": [
    "### def `get_page`:\n",
    "**Args:** \n",
    "* url: A URL on a search page that will be scraped using the `request` Python package.\n",
    "* headers: The pre-set headers used to make the requests look more human.\n",
    "\n",
    "**Returns:**\n",
    "* Either a `BeautifulSoup` soup object of the webpage, or a None object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url, headers=headers):\n",
    "        \n",
    "    try:\n",
    "        r = requests.get(url, headers=headers)\n",
    "        text = r.text\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        return soup\n",
    "    except:\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers)\n",
    "            text = r.text\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            return soup\n",
    "        except:\n",
    "            try:\n",
    "                r = requests.get(url, headers=headers)\n",
    "                text = r.text\n",
    "                soup = BeautifulSoup(text, 'html.parser')\n",
    "                return soup\n",
    "            except:\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb38ac",
   "metadata": {},
   "source": [
    "### def `get_json`:\n",
    "**Args:** \n",
    "* soup: a `BeautifulSoup` object.\n",
    "\n",
    "**Returns:**\n",
    "* A JSON-ified object with the section of the datastructure where we'll scrape data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(soup):\n",
    "    try:\n",
    "        string = str(soup.html.find_all('script')[-1])\n",
    "        string_filtered = string[84:-9]\n",
    "        json_obj = json.loads(string_filtered)\n",
    "\n",
    "        obj = json_obj['niobeMinimalClientData'][0][1]['data']['presentation']['stayProductDetailPage']['sections']['sections']\n",
    "\n",
    "        return obj\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a041b00",
   "metadata": {},
   "source": [
    "### def `fiddle_with_it`:\n",
    "\n",
    "_Each AirBnB listing can have slightly different positions within a section of elements. This is an effective, albeit a little inefficient, way of fiddling with the JSON object if the data returned for a specific feature doesn't look like it should._\n",
    "\n",
    "**Args:** \n",
    "* obj: the JSON-ified object.\n",
    "* var: the variable being scraped.\n",
    "\n",
    "**Returns:**\n",
    "* The scraped value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiddle_with_it(obj, var):\n",
    "    try:\n",
    "        for i in range(25):\n",
    "            try:\n",
    "                if var == 'n_guests':\n",
    "                    value = obj[i]['section']['detailItems'][0]['title'].split(' ')[0]\n",
    "                elif var == 'n_bedrooms':\n",
    "                    value = obj[i]['section']['detailItems'][1]['title'].split(' ')[0]\n",
    "                elif var == 'n_beds':\n",
    "                    value = obj[i]['section']['detailItems'][2]['title'].split(' ')[0]\n",
    "                elif var == 'n_baths':\n",
    "                    value = obj[i]['section']['detailItems'][3]['title'].split(' ')[0]\n",
    "                elif var == 'n_amenities':\n",
    "                    value = obj[i]['section']['seeAllAmenitiesButton']['title']\n",
    "                    if 'ameni' not in value:\n",
    "                        pass\n",
    "                elif var == 'amenities':\n",
    "                    amenities_dict = {}\n",
    "                    amenities = obj[i]['section']['seeAllAmenitiesGroups']\n",
    "                    for x in amenities:\n",
    "                        # Add dictionary value counting number of amenities in that type.\n",
    "                        count = 0\n",
    "                        amenity_type = x['title']\n",
    "                        for y in x['amenities']:\n",
    "                            count += 1\n",
    "                        amenities_dict[amenity_type] = count\n",
    "                    value            = amenities_dict\n",
    "                elif var == 'rating_cleanliness':\n",
    "                    value = obj[i]['section']['ratings'][0]['localizedRating']\n",
    "                elif var == 'rating_communication':\n",
    "                    value = obj[i]['section']['ratings'][2]['localizedRating']\n",
    "                elif var == 'rating_checkin':\n",
    "                    value = obj[i]['section']['ratings'][4]['localizedRating']\n",
    "                elif var == 'rating_accuracy':\n",
    "                    value = obj[i]['section']['ratings'][1]['localizedRating']\n",
    "                elif var == 'rating_location':\n",
    "                    value = obj[i]['section']['ratings'][3]['localizedRating']\n",
    "                elif var == 'rating_value':\n",
    "                    value = obj[i]['section']['ratings'][5]['localizedRating']\n",
    "                elif var == 'sleeping':\n",
    "                    # create a list.\n",
    "                    sleep_list = []\n",
    "                    # Get sleeping options.\n",
    "                    sleep = obj[i]['section']['arrangementDetails']\n",
    "                    # For each option:\n",
    "                    for x in sleep:\n",
    "                        # add a tuple of room and bedtype to the list.\n",
    "                        title = x['title']\n",
    "                        subtitle = x['subtitle']\n",
    "                        sleep_list.append((title, subtitle))\n",
    "            \n",
    "                    value             = sleep_list\n",
    "                elif var == 'description':\n",
    "                    value = obj[i]['section']['htmlDescription']['htmlText']\n",
    "                \n",
    "                return value\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74286634",
   "metadata": {},
   "source": [
    "### def `extract_listing_info`:\n",
    "**Args:** \n",
    "* obj: a JSON data structure object that is parsed out of a `BeautifulSoup` soup object of a webpage.\n",
    "* listing_id: the listing_id of the listing being scraped.\n",
    "* df: a DataFrame with all of the listings scraped to date. For the first listing, it is the empty DataFrame generated by `reset_dataframe()`. For the second listing and onwards, it is a DataFrame of all the prior data scraped.\n",
    "\n",
    "**Returns:**\n",
    "* a DataFrame of all the listings scraped up to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_listing_info(obj, listing_id, df):\n",
    "    try:\n",
    "        try:\n",
    "            listing_id           = listing_id\n",
    "        except:\n",
    "            listing_id           = None\n",
    "        try:\n",
    "            n_guests             = obj[11]['section']['detailItems'][0]['title'].split(' ')[0]\n",
    "        except:\n",
    "            n_guests             = fiddle_with_it(obj, 'n_guests')\n",
    "        try:\n",
    "            n_bedrooms           = obj[11]['section']['detailItems'][1]['title'].split(' ')[0]\n",
    "        except:\n",
    "            n_bedrooms           = fiddle_with_it(obj, 'n_bedrooms')\n",
    "        try:\n",
    "            n_beds               = obj[11]['section']['detailItems'][2]['title'].split(' ')[0]\n",
    "        except:\n",
    "            n_beds               = fiddle_with_it(obj, 'n_beds')\n",
    "        try:\n",
    "            n_baths              = obj[11]['section']['detailItems'][3]['title'].split(' ')[0]\n",
    "        except:\n",
    "            n_baths              = fiddle_with_it(obj, 'n_baths')\n",
    "        try:\n",
    "            n_amenities          = obj[16]['section']['seeAllAmenitiesButton']['title']\n",
    "        except:\n",
    "            n_amenities          = fiddle_with_it(obj, 'n_amenities')\n",
    "        try:\n",
    "            # ccreate a dictionary.\n",
    "            amenities_dict = {}\n",
    "            # Get all of the nested amenities.\n",
    "            amenities = obj[16]['section']['seeAllAmenitiesGroups']\n",
    "            # For each group of amenities.\n",
    "            for x in amenities:\n",
    "                # Add dictionary value counting number of amenities in that type.\n",
    "                count = 0\n",
    "                amenity_type = x['title']\n",
    "                for y in x['amenities']:\n",
    "                    count += 1\n",
    "                amenities_dict[amenity_type] = count\n",
    "    \n",
    "            amenities            = amenities_dict\n",
    "        except:\n",
    "            amenities            = fiddle_with_it(obj, 'amenities')\n",
    "        try:\n",
    "            rating_cleanliness   = obj[1]['section']['ratings'][0]['localizedRating']\n",
    "        except:\n",
    "            rating_cleanliness   = fiddle_with_it(obj, 'rating_cleanliness')\n",
    "        try:\n",
    "            rating_communication = obj[1]['section']['ratings'][2]['localizedRating']\n",
    "        except:\n",
    "            rating_communication = fiddle_with_it(obj, 'rating_communication')\n",
    "        try:\n",
    "            rating_checkin       = obj[1]['section']['ratings'][4]['localizedRating']\n",
    "        except:\n",
    "            rating_checkin       = fiddle_with_it(obj, 'rating_checkin')\n",
    "        try:\n",
    "            rating_accuracy      = obj[1]['section']['ratings'][1]['localizedRating']\n",
    "        except:\n",
    "            rating_accuracy      = fiddle_with_it(obj, 'rating_accuracy')\n",
    "        try:\n",
    "            rating_location      = obj[1]['section']['ratings'][3]['localizedRating']\n",
    "        except:\n",
    "            rating_location      = fiddle_with_it(obj, 'rating_location')\n",
    "        try:\n",
    "            rating_value         = obj[1]['section']['ratings'][5]['localizedRating']\n",
    "        except:\n",
    "            rating_value         = fiddle_with_it(obj, 'rating_value')\n",
    "        try:\n",
    "            # create a list.\n",
    "            sleep_list = []\n",
    "            # Get sleeping options.\n",
    "            sleep = obj[15]['section']['arrangementDetails']\n",
    "            # For each option:\n",
    "            for x in sleep:\n",
    "                # add a tuple of room and bedtype to the list.\n",
    "                title = x['title']\n",
    "                subtitle = x['subtitle']\n",
    "                sleep_list.append((title, subtitle))\n",
    "            \n",
    "            sleeping             = sleep_list\n",
    "        except:\n",
    "            sleeping             = fiddle_with_it(obj, 'sleeping')\n",
    "        try:\n",
    "            description          = obj[14]['section']['htmlDescription']['htmlText']\n",
    "        except:\n",
    "            description          = fiddle_with_it(obj, 'description')\n",
    "                    \n",
    "        row = pd.DataFrame(\n",
    "            {'listing_id':           [listing_id],\n",
    "             'n_guests':             [n_guests],\n",
    "             'n_bedrooms':           [n_bedrooms],\n",
    "             'n_beds':               [n_beds],\n",
    "             'n_baths':              [n_baths],\n",
    "             'n_amenities':          [n_amenities],\n",
    "             'amenities':            [amenities],\n",
    "             'rating_cleanliness':   [rating_cleanliness],\n",
    "             'rating_communication': [rating_communication],\n",
    "             'rating_checkin':       [rating_checkin],\n",
    "             'rating_accuracy':      [rating_accuracy],\n",
    "             'rating_location':      [rating_location],\n",
    "             'rating_value':         [rating_value],\n",
    "             'sleeping':             [sleeping],\n",
    "             'description':          [description]\n",
    "            })\n",
    "        # Return dataframe, with new row of data appended.\n",
    "        return pd.concat([df, row], ignore_index=True)\n",
    "    except:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54ea54",
   "metadata": {},
   "source": [
    "# Generate a list of tuples that will be used to iterate through listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_and_urls = [(val[0], val[1]) for val in zip(df['listing_id'], df['listing_url'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d72d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ids_and_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84232d4",
   "metadata": {},
   "source": [
    "# Start scraping the webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00078456",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_count = 0\n",
    "\n",
    "pages_dataframe = reset_dataframe()\n",
    "\n",
    "extract1 = pd.read_csv('extract pages 1.csv')\n",
    "\n",
    "for id_and_url in tqdm(ids_and_urls[59500+93780:]):\n",
    "    id_ = id_and_url[0]\n",
    "    url = id_and_url[1]\n",
    "    soup = get_page(url)\n",
    "    obj = get_json(soup)\n",
    "    pages_dataframe = extract_listing_info(obj, id_, pages_dataframe)\n",
    "    url_count += 1\n",
    "    time.sleep(random.randint(4,7))\n",
    "    if url_count % 100 == 0:\n",
    "        # Copy the list of frames.\n",
    "        #write_frames = frames.copy()\n",
    "        # Add the extracted listings from prior jobs that were interrupted.\n",
    "        #write_frames.append(extract1)\n",
    "        # Write the concatenated list of frames into one csv that is plugged into Tableau for monitoring.\n",
    "        pd.concat([pages_dataframe, extract1], ignore_index=True).to_csv('scraped_pages.csv')\n",
    "        #pages_dataframe.to_csv('scraped_pages.csv') \n",
    "\n",
    "pages_dataframe = pd.concat([pages_dataframe, extract1], ignore_index=True)\n",
    "        \n",
    "pages_dataframe.to_csv('scraped_pages.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2938eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "pages_dataframe.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdc05d",
   "metadata": {},
   "source": [
    "# Fiddle and fix the data.\n",
    "\n",
    "Scraping this one got a little tricky. It required getting creative with VPNs. Because of this, I had to regularly stop the main job, drop nulls, and start again. Nulls are a sign that AirBnB blocked the scraper, and that we need to switch to a different IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0884b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data.\n",
    "frame = pd.read_csv('scraped_pages extract 1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea56b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop index.\n",
    "frame.drop(\n",
    "    columns=[ 'Unnamed: 0'],\n",
    "    inplace=True\n",
    ")\n",
    "# Drop dupes.\n",
    "frame.drop_duplicates(\n",
    "    subset=['listing_id'], \n",
    "    inplace=True, \n",
    "    ignore_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the info in the data.\n",
    "frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records that have n_guests null -- this is our proxy for listings that didn't get scraped.\n",
    "frame.dropna(axis=0, subset=['n_guests'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75041e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the info in the table again.\n",
    "frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the de-nulled data.\n",
    "frame.to_csv('scraped_pages extract 1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data.\n",
    "frame = pd.read_csv('scraped_pages extract 1.csv')\n",
    "frame.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21759bfb",
   "metadata": {},
   "source": [
    "## Get the listing ids that aren't yet scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in main data. \n",
    "df = pd.read_csv('scraped_listings BACKUP.csv')\n",
    "df.drop(columns=['Unnamed: 0','Unnamed: 0.8','Unnamed: 0.9'],inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c31ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['listing_id']))\n",
    "print(len(frame['listing_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69eca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ids of listings scraped in part 1.\n",
    "search_page_ids = [id_ for id_ in df['listing_id'].astype('str')]\n",
    "# Get ids of listings scraped in part 2 so far.\n",
    "listing_page_ids = [id_ for id_ in frame['listing_id'].astype('int').astype('str')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IDs left to be scraped.\n",
    "ids_left_to_scrape = [id_ for id_ in tqdm(search_page_ids) if id_ not in listing_page_ids]\n",
    "print(len(ids_left_to_scrape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URLs.\n",
    "urls_left_to_scrape = ['https://www.airbnb.com/rooms/' + id_ for id_ in ids_left_to_scrape]\n",
    "urls_left_to_scrape[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327e652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuples to loop through.\n",
    "ids_and_urls = [(val[0], val[1]) for val in zip(ids_left_to_scrape, urls_left_to_scrape)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing something...\n",
    "random.shuffle(ids_and_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c126ad2",
   "metadata": {},
   "source": [
    "# Restart scrapper with fixed details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_count = 0\n",
    "\n",
    "pages_dataframe = reset_dataframe()\n",
    "\n",
    "extract1 = pd.read_csv('scraped_pages extract 1.csv')\n",
    "extract1.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "\n",
    "for id_and_url in tqdm(ids_and_urls):\n",
    "    id_ = id_and_url[0]\n",
    "    url = id_and_url[1]\n",
    "    soup = get_page(url)\n",
    "    obj = get_json(soup)\n",
    "    pages_dataframe = extract_listing_info(obj, id_, pages_dataframe)\n",
    "    url_count += 1\n",
    "    time.sleep(random.randint(3,6))\n",
    "    if url_count % 100 == 0:\n",
    "        # Copy the list of frames.\n",
    "        #write_frames = frames.copy()\n",
    "        # Add the extracted listings from prior jobs that were interrupted.\n",
    "        #write_frames.append(extract1)\n",
    "        # Write the concatenated list of frames into one csv that is plugged into Tableau for monitoring.\n",
    "        pd.concat([pages_dataframe, extract1], ignore_index=True).to_csv('scraped_pages.csv')\n",
    "        #pages_dataframe.to_csv('scraped_pages.csv') \n",
    "\n",
    "pages_dataframe = pd.concat([pages_dataframe, extract1], ignore_index=True)\n",
    "        \n",
    "pages_dataframe.to_csv('scraped_pages.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cdff1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
