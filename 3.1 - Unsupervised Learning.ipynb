{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53eb5b9a",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "Now that we have the data, let's figure out if we can pull out some features from it that can help us classify the types of data that it is.\n",
    "\n",
    "In this notebook, we will:\n",
    "* Use `umap` and `Kmeans` to find potential clusters for the listing data that can be used for isolating and modeling different chunks of the data.\n",
    "* Use `PCA` to reduce all of the county and state level data into a few dimensions that represent the majority of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b0f3b",
   "metadata": {},
   "source": [
    "# Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dimensionality reduction: preprocessing.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Dimensionality reduction: PCA.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Clustering.\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Dimensionality reduction: UMAP.\n",
    "import umap.umap_ as umap\n",
    "from umap import utils\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# visualization.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Progress tracking.\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade586a",
   "metadata": {},
   "source": [
    "# 0. Import Data and filter out nulls.\n",
    "Read in the cleaned data from the earlier notebooks. We'll make a small tweak to the data in `airbnb_df` before proceeding with our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48586c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "airbnb_df = pd.read_csv('CLEANED airbnb_data.csv').drop(columns=['Unnamed: 0'])\n",
    "county_df = pd.read_csv('CLEANED county_data.csv').drop(columns=['Unnamed: 0'])\n",
    "parks_df = pd.read_csv('CLEANED natl_state_park_by_us_state.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f3175",
   "metadata": {},
   "source": [
    "### 0.0 Drop listings with null values.\n",
    "\n",
    "A decent chunk of the listings we scraped were unable to get any data on the listing's page. With those listings, we only have data on the search pages.\n",
    "\n",
    "if we throw out all listings that don't have at least some data from the listing's page, we'll reduce the amount of data we need to impute. This should help increase the odds of finding latent, distinct clusters existing within the data when we go to cluster it.\n",
    "\n",
    "* Asess `len()` of `airbnb_df`.\n",
    "* Save copy of `airbnb_df` as `original` before dropping records.\n",
    "* Drop all records where `n_guests` is null (proxy for not being able to access the listing's page).\n",
    "* Asess `len()` of `airbnb_df` after dropping records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d857c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the results before scraping.\n",
    "print('# of listings:',len(airbnb_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the pre-filtered data for comparision after filtering.\n",
    "original = airbnb_df.copy()\n",
    "\n",
    "# Drop listings where n_guests is null. This is a good proxy for not being able to scrape any data\n",
    "# on the listing's webpage.\n",
    "airbnb_df.dropna(axis=0, subset=['n_guests'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the results after scraping.\n",
    "print('# of listings:',len(airbnb_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165f600",
   "metadata": {},
   "source": [
    "### 0.I Visually assess filtered data.\n",
    "\n",
    "Although we've dropped 45% of the data, we still have just over 100,000 records. Looking at the visual comparision of `original` and `airbnb_df` side by side, the filtered data seems to be drawing reasonably from the distribution without over indexing on listings from any one area of the country.\n",
    "\n",
    "* Create subplot with `plt.subplots(1,2)`.\n",
    "* On the first subplot, show `original` data, where each dot is a listing.\n",
    "* On the second subplot, show `airbnb_df` data, where each dot is a listing.\n",
    "* Render the plot with `plt.show`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2919f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick side by side chart showing the data, before and after.\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "# Sizing.\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "#plot 1:\n",
    "axs[0].scatter(original['longitude'],original['latitude'],s=1,alpha=.05)\n",
    "axs[0].set_title('Original Data ({} listings)'.format(len(original)))\n",
    "axs[0].set_xlabel('longitude')\n",
    "axs[0].set_ylabel('latitude')\n",
    "\n",
    "#plot 2:\n",
    "axs[1].scatter(airbnb_df['longitude'],airbnb_df['latitude'], s=1,alpha=.05)\n",
    "axs[1].set_title('Filtered Data ({} listings)'.format(len(airbnb_df)))\n",
    "axs[1].set_xlabel('longitude')\n",
    "axs[1].set_ylabel('latitude')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1fcc63",
   "metadata": {},
   "source": [
    "# I. Clustering on listing attributes.\n",
    "\n",
    "Using the features scraped off of AirBnB's website, we are going to see if we can draw out any structures of interest from the data. The goal would be to see if we could find either reduced dimensions features for supervised learning predictions, or clusters that could be used to partition data for different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69ff27b",
   "metadata": {},
   "source": [
    "### I.I Pre-process Data.\n",
    "\n",
    "For our unsupervised learning, we'll need to change this data into a shape that can be accepted as an input for machine learning. First, we'll drop all columns that aren't numeric (or numeric columns that we don't think are predictors, like `postcode`, `latitude`, `longitude`). Then, we will normalize the data. While normalizing the data isn't technically required for the data to be in acceptable state for unsupervised machine learning, skipping this step can weaken the performance of the model. Scaling all data down to the same distribution makes it easier to pick out patterns, and not have the algorithm trip over features that have differences in their scale.\n",
    "\n",
    "* Drop columns that won't be used for clustering.\n",
    "* Use `StandardScaler` to normalize data.\n",
    "* Use `SimpleImputer` to impute the median for missing values (there are still some of these in the records we have left).\n",
    "* Return `x_normalized` for unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f987fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up PCA model - airbnb.\n",
    "\n",
    "# copy data.\n",
    "pca_airbnb_df = airbnb_df.copy()\n",
    "\n",
    "# columns that are not int/floats, or that deal with geography.\n",
    "dropcols = ['listing_url','country','state','city','county','postcode','listing_title',\n",
    "       'latitude','longitude','description','image_1','image_2','image_3','image_4','image_5']\n",
    "\n",
    "# drop columns.\n",
    "pca_airbnb_df.drop(columns=dropcols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4810ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data. Remove listing id (just and ID) and price (leaks data to supervised learning model).\n",
    "y = pca_airbnb_df[['listing_id','price']]\n",
    "X = pca_airbnb_df.drop(columns=['listing_id','price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de03f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = StandardScaler().fit(X).transform(X)\n",
    "\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "X_normalized = imp_median.fit_transform(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c7b45",
   "metadata": {},
   "source": [
    "### I.III Visually assess normalized data.\n",
    "\n",
    "We'll take a quick look at two variables, `price` and `n_amenities`, and see how normalization changed the relationship between the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7932da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick side by side chart showing the data, before and after.\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "# Sizing.\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "#plot 1:\n",
    "axs[0].scatter(pca_airbnb_df['price'],pca_airbnb_df['n_amenities'],s=1,alpha=.05)\n",
    "axs[0].set_title('Unnormalized Data')\n",
    "axs[0].set_xlabel('Price')\n",
    "axs[0].set_ylabel('# Amenities')\n",
    "\n",
    "#plot 2:\n",
    "axs[1].scatter(X_normalized[:,10],X_normalized[:,22], s=1,alpha=.05)\n",
    "axs[1].set_title('Normalized Data')\n",
    "axs[1].set_xlabel('Price')\n",
    "axs[1].set_ylabel('# Amenities')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086703c",
   "metadata": {},
   "source": [
    "### I.IV PCA\n",
    "\n",
    "Principal Component Analysis can help us try to find new features that reduce the variance of multiple other variables into fewer dimensions.\n",
    "\n",
    "* Create a PCA model called `pca` that gets the 10 nearest components.\n",
    "* Visually assess the first two dimensions.\n",
    "* Plot all 10 components and their relationship between different variables.\n",
    "* Plot the explained variance for each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 10, random_state=0).fit(X_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_values = PCA(n_components = 10).fit_transform(X_normalized)\n",
    "\n",
    "plt.scatter(pca_values[:,0], pca_values[:,1])\n",
    "plt.title('First two dimensions: PCA')\n",
    "plt.xlabel('2nd dimension')\n",
    "plt.ylabel('1st dimension')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(pca, top_k = 2):\n",
    "    pca = PCA(n_components = 10).fit(X_normalized)\n",
    "    fig, ax = plt.subplots(figsize=(18, 12))\n",
    "    plt.imshow(pca.components_[0:top_k], interpolation = 'none', cmap = 'plasma', aspect='auto')\n",
    "    labels= np.round(pca.components_[0:top_k],1)\n",
    "    sns.heatmap(pca.components_[0:top_k], cmap=\"plasma\", cbar=False, annot=labels, annot_kws={'fontsize': 8})\n",
    "    feature_names=list(X.columns)\n",
    "    plt.xticks(np.arange(-0., len(feature_names), 1) , feature_names, rotation = 90, fontsize=12)\n",
    "    plt.yticks(np.arange(0., 10, 1), ['1st PC', '2nd PC','3rd PC','4th PC', '5th PC', \n",
    "                                     '6th PC','7th PC','8th PC','9th PC', '10th PC'],rotation=90, fontsize = 16)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    \n",
    "plot_pca(pca, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002e452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'explained var ratio':pca.explained_variance_ratio_,\n",
    "                   'PC':['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']})\n",
    "sns.barplot(x = 'PC',y = \"explained var ratio\", data = df, color=\"c\");\n",
    "np.cumsum(df['explained var ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf1544",
   "metadata": {},
   "source": [
    "### Analysis of PCA on AirBnB listing data.\n",
    "\n",
    "PCA did not do a strong job in explaining the variance of all the data in `airbnb_df`. The top 10 components explained a total of 54.5% of the variance in the data. I would have liked to see closer to 70-80% by the 5th component.\n",
    "\n",
    "We weren't able to pull out reduced dimensions for features from `airbnb_df`. However, we might still be able to extract clusters using other techniques. Clustering will be important for supervised machine learning, as we can use the clusters to partition data into separate models and potentially increase the ability to predict a listings price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388218a",
   "metadata": {},
   "source": [
    "### I.V Generating UMAP Embeddings\n",
    "\n",
    "Uniform Manifold Approximation and Projection, or `umap`, can be used for non-linear dimensionality reduction. We might be able to identify some strong `Kmeans` clusters on top of this, but first we need to do a little hyperparameter tuning.\n",
    "\n",
    "* Creating a list with each unique combination of the following metrics and neighbors: metrics: (`mahalanobis`, `wminkowski`, `cosine`, `correlation`), neighbors: (`100`, `300`, `500`).\n",
    "* For each combination, extract the 2D embeddings for each model.\n",
    "* Save the embedding locally in a file that will look like `listing_cosine_100.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d54e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish metrics and neighbors.\n",
    "metrics = ['mahalanobis', 'wminkowski', 'cosine', 'correlation']\n",
    "neighbors = [100, 300, 500]\n",
    "\n",
    "# Empty list for model parameters.\n",
    "model_parameters = []\n",
    "\n",
    "# Append each unique metric-neighbor combination to model_parameters.\n",
    "for metric in metrics:\n",
    "    for neighbor in neighbors:\n",
    "        model_parameters.append((metric, neighbor))\n",
    "\n",
    "# Check outputs.\n",
    "print(model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e655bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each tuple in model_parameters.\n",
    "for parameters in model_parameters:\n",
    "    \n",
    "    # Extract the metric and the neighbor values.\n",
    "    metric = parameters[0]\n",
    "    neighbors = parameters[1]\n",
    "    \n",
    "    # Extract embeddings for X_normalized using the metric and neighbors.\n",
    "    embedding = umap.UMAP(n_neighbors=neighbors,\n",
    "                          min_dist=0.0,\n",
    "                          random_state=0,\n",
    "                          metric=metric,\n",
    "                          verbose=True\n",
    "                    ).fit_transform(\n",
    "                          X_normalized\n",
    "                    )\n",
    "    \n",
    "    # Save the outputs of the model to prevent overloading computer memory.\n",
    "    np.savetxt('listing_{}_{}.csv'.format(metric, neighbors),embedding, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f03c1",
   "metadata": {},
   "source": [
    "### I.VI UMAP Performance\n",
    "\n",
    "We can now to start to assess how well these models performed in creating separated clusters. This can give us a sense of which model(s) had the best performance breaking apart the `umap` embeddings into distinct groups.\n",
    "\n",
    "* Read in all of the csv files created by the different `umap` model variants.\n",
    "* Test `KMeans` clustering models on each variant. Test between 2 and 11 clusters.\n",
    "* Select the cluster value with the highest silhouette score for that model variant (a measurement of intra-cluster cohesiveness and inter-cluster separation).\n",
    "* Visualize optimal clusters for that `umap` model in a chart, showing the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all embeddings that were generated in the prior step.\n",
    "mahalanobis_100 = pd.read_csv('listing_mahalanobis_100.csv',header=None,names=['x','y'])\n",
    "mahalanobis_300 = pd.read_csv('listing_mahalanobis_300.csv',header=None,names=['x','y'])\n",
    "mahalanobis_500 = pd.read_csv('listing_mahalanobis_500.csv',header=None,names=['x','y'])\n",
    "wminkowski_100 = pd.read_csv('listing_wminkowski_100.csv',header=None,names=['x','y'])\n",
    "wminkowski_300 = pd.read_csv('listing_wminkowski_300.csv',header=None,names=['x','y'])\n",
    "wminkowski_500 = pd.read_csv('listing_wminkowski_500.csv',header=None,names=['x','y'])\n",
    "cosine_100 = pd.read_csv('listing_cosine_100.csv',header=None,names=['x','y'])\n",
    "cosine_300 = pd.read_csv('listing_cosine_300.csv',header=None,names=['x','y'])\n",
    "cosine_500 = pd.read_csv('listing_cosine_500.csv',header=None,names=['x','y'])\n",
    "correlation_100 = pd.read_csv('listing_correlation_100.csv',header=None,names=['x','y'])\n",
    "correlation_300 = pd.read_csv('listing_correlation_300.csv',header=None,names=['x','y'])\n",
    "correlation_500 = pd.read_csv('listing_correlation_500.csv',header=None,names=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_chart(df,title,optimal_clusters=None,score=None):\n",
    "    \n",
    "    if optimal_clusters==None:\n",
    "        # Empty list for scores.\n",
    "        silhouette_coefficients = []\n",
    "\n",
    "        # Loop through cluster values between 2 and 10.\n",
    "        for k in tqdm(range(2, 11)):\n",
    "\n",
    "            # Create Kmeans models.\n",
    "            kmeans = KMeans(\n",
    "                init=\"random\",\n",
    "                n_clusters=k,\n",
    "                n_init=10,\n",
    "                max_iter=300,\n",
    "                random_state=0\n",
    "            )\n",
    "\n",
    "            # Fit model.\n",
    "            kmeans.fit(df[['x','y']])\n",
    "\n",
    "            # Get silhouette score, append to list.\n",
    "            score = silhouette_score(df[['x','y']], kmeans.labels_)\n",
    "            silhouette_coefficients.append(score)\n",
    "\n",
    "        # Identify optimal clusters for model variant.\n",
    "        max_value = max(silhouette_coefficients)\n",
    "        max_index = [index for index, item in enumerate(silhouette_coefficients) if item == max_value][0]\n",
    "        optimal_clusters = range(2,11)[max_index]\n",
    "    \n",
    "    # Generate optimal clusters Kmeans.\n",
    "    kmeans = KMeans(\n",
    "            init=\"random\",\n",
    "            n_clusters=optimal_clusters,\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            random_state=0\n",
    "        )\n",
    "    \n",
    "    # Fit model.\n",
    "    kmeans.fit(df[['x','y']])\n",
    "    \n",
    "    # Save label to embeddings DataFrame.\n",
    "    df['label'] = kmeans.labels_.astype('str')\n",
    "    \n",
    "    # Create chart to show clustering and silhouette score.\n",
    "    chart = alt.Chart(df).mark_point(\n",
    "        filled=True,\n",
    "        size=10\n",
    "    ).encode(\n",
    "        x='x',\n",
    "        y='y',\n",
    "        color='label:N'\n",
    "    ).properties(\n",
    "        title=(title, 'Optimal Clusters={},Silhouette Score={}'.format(optimal_clusters, score)),\n",
    "        width=200,\n",
    "        height=250\n",
    "    )\n",
    "    \n",
    "    # Return chart.\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate charts for Mahalanobis variants.\n",
    "m100 = gen_chart(mahalanobis_100,'Metric: Mahalanobis; Neighbors: 100',optimal_clusters=None)\n",
    "m300 = gen_chart(mahalanobis_300,'Metric: Mahalanobis; Neighbors: 300',optimal_clusters=None)\n",
    "m500 = gen_chart(mahalanobis_500,'Metric: Mahalanobis; Neighbors: 500',optimal_clusters=None)\n",
    "\n",
    "# Generate charts for Wminkowski variants.\n",
    "wm100 = gen_chart(wminkowski_100,'Metric: Wminkowski; Neighbors: 100',optimal_clusters=None)\n",
    "wm300 = gen_chart(wminkowski_300,'Metric: Wminkowski; Neighbors: 300',optimal_clusters=None)\n",
    "wm500 = gen_chart(wminkowski_500,'Metric: Wminkowski; Neighbors: 500',optimal_clusters=None)\n",
    "\n",
    "# Generate charts for Cosine variants.\n",
    "cos100 = gen_chart(cosine_100,'Metric: Cosine; Neighbors: 100',optimal_clusters=None)\n",
    "cos300 = gen_chart(cosine_300,'Metric: Cosine; Neighbors: 300',optimal_clusters=None)\n",
    "cos500 = gen_chart(cosine_500,'Metric: Cosine; Neighbors: 500',optimal_clusters=None)\n",
    "\n",
    "# Generate charts for Correlation variants.\n",
    "cor100 = gen_chart(correlation_100,'Metric: Correlation; Neighbors: 100',optimal_clusters=None)\n",
    "cor300 = gen_chart(correlation_300,'Metric: Correlation; Neighbors: 300',optimal_clusters=None)\n",
    "cor500 = gen_chart(correlation_500,'Metric: Correlation; Neighbors: 500',optimal_clusters=None)\n",
    "\n",
    "# Create concatenated chart.\n",
    "alt.vconcat(\n",
    "    alt.hconcat(m100,m300,m500),\n",
    "    alt.hconcat(wm100,wm300,wm500),\n",
    "    alt.hconcat(cos100,cos300,cos500),\n",
    "    alt.hconcat(cor100,cor300,cor500)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff406b",
   "metadata": {},
   "source": [
    "### Analysis of UMAP Performance\n",
    "\n",
    "The models performed between 0.405 and 0.515. Since the range of a silhouette score can be between -1 and 1, these clusters are information but perhaps not definitive clusters. However, there might be something here that can be used for supervised learning.\n",
    "\n",
    "* **Top Mahalanobis model:** 100 neighbors, 2 clusters _(silhouette score: .453)_\n",
    "* **Top Wminkowski model:** 500 neighbors, 7 clusters _(silhouette score: .405)_\n",
    "* **Top Cosine model:** 100 neighbors, 2 clusters _(silhouette score: .499)_\n",
    "* **Top Correlation model:** 100 neighbors, 2 clusters _(silhouette score: .515)_\n",
    "* **Top Overall Model:** Correlation, 100 neighbors, 2 clusters _(silhouette score: .515)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d50c7",
   "metadata": {},
   "source": [
    "### I.VII Save top model for each metric to data.\n",
    "\n",
    "We'll take the neighbor value for each metric, cluster it by the `Kmeans` with the highest silhouette score, and save it as a column in the `airbnb_df` DataFrame.\n",
    "\n",
    "* Function for returning labels for specific embedding / k clusters combination.\n",
    "* Save top model for each metric's labels as a new column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1032d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cluster_labels(df, k):\n",
    "    \n",
    "    # Create kmeans algorithm.\n",
    "    kmeans = KMeans(\n",
    "            init=\"random\",\n",
    "            n_clusters=k,\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            random_state=0\n",
    "        )\n",
    "\n",
    "    # Fit to dataset.\n",
    "    kmeans.fit(df[['x','y']])\n",
    "    \n",
    "    # Return labels.\n",
    "    return kmeans.labels_.astype('str')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dfd46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save labels to new columns of DataFrames.\n",
    "airbnb_df['mahalanobis_100'] = add_cluster_labels(df=mahalanobis_100,k=2)\n",
    "airbnb_df['wminkowski_500'] = add_cluster_labels(df=wminkowski_500,k=7)\n",
    "airbnb_df['cos100'] = add_cluster_labels(df=cosine_100,k=2)\n",
    "airbnb_df['cor100'] = add_cluster_labels(df=correlation_100,k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac970f25",
   "metadata": {},
   "source": [
    "### I.VIII Visualize elements with highest correlation to price in each cluster for each model.\n",
    "\n",
    "We want to get some intuitive sense of what each cluster in each model might be picking up on, and how those trends might correlate to the price of the listing.\n",
    "\n",
    "* Plot each of the top models, and how each variable correlates to price.\n",
    "* Analyze trends that exist within each cluster of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e484a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap(col, airbnb_df=airbnb_df):\n",
    "    \n",
    "    # Create copy to prevent mutating the original DataFrame.\n",
    "    df = airbnb_df.copy()\n",
    "    \n",
    "    # columns that are not int/floats, or that deal with geography.\n",
    "    dropcols = ['listing_url','country','state','city','county','postcode','listing_title',\n",
    "       'latitude','longitude','description','image_1','image_2','image_3','image_4','image_5',\n",
    "               'listing_id','mahalanobis_100','wminkowski_500','cos100','cor100']\n",
    "    \n",
    "    # Exclude the group column from the drop list.\n",
    "    dropcols.remove(col)\n",
    "    \n",
    "    # drop columns.\n",
    "    df.drop(columns=dropcols, inplace=True)\n",
    "    \n",
    "    # Empty variable to build chart on.\n",
    "    concat_chart = None\n",
    "    \n",
    "    # Loop through each cluster.\n",
    "    for val in sorted(df[col].unique()):\n",
    "        \n",
    "        # Create a copy to prevent mutating the DataFrame `df`.\n",
    "        frame = df.copy()\n",
    "        \n",
    "        # Filter for only that cluster.\n",
    "        frame = frame[frame[col] == val]\n",
    "        \n",
    "        # Get a correlation matrix, sorted by variables that correlate with price.\n",
    "        corr = frame.corr().sort_values(by='price')\n",
    "        corr.drop(index='price',inplace=True)\n",
    "        \n",
    "        # Create a chart showing the correlations with price.\n",
    "        chart = alt.Chart(corr.reset_index()).mark_bar().encode(\n",
    "            x=alt.X('index',sort='y', title='Feature'),\n",
    "            y=alt.Y('price',title='Correlation with Price')\n",
    "        ).properties(\n",
    "            title='Price Correlations for {}, cluster {}'.format(col, val))\n",
    "        \n",
    "        # If concat_chart is None.\n",
    "        if concat_chart == None:\n",
    "            # Replace it with the chart.\n",
    "            concat_chart = chart\n",
    "        # If there is already a chart,\n",
    "        else:\n",
    "            # Vertically concatenate the new chart to the old chart.\n",
    "            concat_chart = concat_chart & chart\n",
    "    \n",
    "    # Return concetenated chart.\n",
    "    return concat_chart.resolve_scale(\n",
    "        x='independent'\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf92545",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap('mahalanobis_100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a178f4",
   "metadata": {},
   "source": [
    "### Analysis: metric=mahalanobis, neighbors=100\n",
    "\n",
    "**cluster 0**\n",
    "* Most positive correlating variables: `n_baths`, `bed_king`, `n_bedrooms`.\n",
    "* Most negative correlating variables: `private_room`, `n_reviews`, `amenities_not_included`.\n",
    "\n",
    "**cluster 1**\n",
    "* Most positive correlating variables: `n_baths`, `n_bedrooms`, `n_beds`. \n",
    "* Most negative correlating variables: `is_superhost`, `n_reviews`, `rating_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d011f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap('wminkowski_500')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907c7a0",
   "metadata": {},
   "source": [
    "### Analysis: metric=Wminkowski, neighbors=500\n",
    "\n",
    "**cluster 0**\n",
    "* Most positive correlating variables: `n_baths`, `entire_home`, `bed_king`.\n",
    "* Most negative correlating variables: `n_ratings`, `private_room`, `rating_value`.\n",
    "\n",
    "**cluster 1**\n",
    "* Most positive correlating variables: `bed_king`, `bed_single`, `bed_queen`.\n",
    "* Most negative correlating variables: `rating_value`, `n_ratings`, `private_room`.\n",
    "\n",
    "**cluster 2**\n",
    "* Most positive correlating variables: `n_baths`, `n_beds`, `n_guests`.\n",
    "* Most negative correlating variables: `amenities_entertainment`, `n_ratings`, `rating_value`.\n",
    "\n",
    "**cluster 3**\n",
    "* Most positive correlating variables: `n_guests`, `n_beds`, `bed_king`.\n",
    "* Most negative correlating variables: `rating_value`, `n_reviews`, `perc_discount`.\n",
    "\n",
    "**cluster 4**\n",
    "* Most positive correlating variables: `n_guests`, `n_beds`, `bed_king`.\n",
    "* Most negative correlating variables: `private_room`, `amenities_safety`, `amenities_bathroom`.\n",
    "\n",
    "**cluster 5**\n",
    "* Most positive correlating variables: `n_baths`, `n_beds`, `bed_king`.\n",
    "* Most negative correlating variables: `private_room`, `n_reviews`, `is_superhost`.\n",
    "\n",
    "**cluster 6**\n",
    "* Most positive correlating variables: `n_baths`, `amenities_entertainment`, `n_bedrooms`.\n",
    "* Most negative correlating variables: `private_room`, `amenities_not_included`, `n_reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap('cos100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d41ff",
   "metadata": {},
   "source": [
    "### Analysis: metric=Cosine, neighbors=100\n",
    "\n",
    "**cluster 0**\n",
    "* Most positive correlating variables: `n_beds`, `bed_king`, `n_bedrooms`.\n",
    "* Most negative correlating variables: `amenities_kitchen_dining`, `amenities_safety`, `amenities_bathroom`.\n",
    "\n",
    "**cluster 1**\n",
    "* Most positive correlating variables: `n_baths`, `bed_king`, `n_bedrooms`.\n",
    "* Most negative correlating variables: `n_reviews`, `is_superhost`, `rating_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a53e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap('cor100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb10997d",
   "metadata": {},
   "source": [
    "### Analysis: metric=Correlation, neighbors=100\n",
    "\n",
    "**cluster 0**\n",
    "* Most positive correlating variables: `n_baths`, `bed_king`, `n_bedrooms`.\n",
    "* Most negative correlating variables: `n_reviews`, `is_superhost`, `rating_value`.\n",
    "\n",
    "**cluster 1**\n",
    "* Most positive correlating variables: `n_guests`, `n_beds`, `bed_king`.\n",
    "* Most negative correlating variables: `amenities_kitchen_dining`, `amentities_safety`, `amenities_bathroom`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f2449",
   "metadata": {},
   "source": [
    "### Analysis: Insights from looking at cluster-specific pricing correlations.\n",
    "\n",
    "* Variables that indicate the size of the listing (`n_baths`, `n_guests`, `n_bedrooms`) consistently have positive correlations with price.\n",
    "* Variables that indicate a smaller loding (`is_private_room`, `amenities_not_included`) consistently have negative correlations with price.\n",
    "* In some clusters, `bed_king` is picked up as a variable that correlates with price. Perhaps this suggests these clusters are picking up on more larger listings with big beds.\n",
    "* Suprisingly, the number of reviews and ratings can sometimes negative correlate with `price`. However, we should exercise caution. Median imputing affect the trends here. There might also be an explanation that much more expensive listings (1K+/night) might have less reviews than cheaper listings (100/night).\n",
    "\n",
    "One thing I might need to watch out for in the supervised learning model is outliers in price. There might be some outliers that affect the way the model comes together. Because these clusters did not look at price data I'm not too worried about reassessing the unsupervised machine learning models, but it might be something to consider in data cleaning for the unsupervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57992f1e",
   "metadata": {},
   "source": [
    "# I.IX Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfd308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_x, correlation_y = [1,2,3], [0.515,0.461,0.471]\n",
    "cosine_x, cosine_y = [1,2,3], [.499,0.487,0.472]\n",
    "mahalanobis_x, mahalanobis_y = [1,2,3], [0.453,0.401,0.411]\n",
    "wminkowski_x, wminkowski_y = [1,2,3], [0.401,0.398,0.405]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.plot(correlation_x, correlation_y,label='correlation')\n",
    "plt.plot(cosine_x, cosine_y,label='cosine')\n",
    "plt.plot(mahalanobis_x, mahalanobis_y,label='mahalanobis')\n",
    "plt.plot(wminkowski_x, wminkowski_y,label='wminkowski')\n",
    "\n",
    "plt.xlabel('Neighbors')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks([1,2,3],['100','300','500'])\n",
    "\n",
    "plt.title('Sensitivity Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1f791",
   "metadata": {},
   "source": [
    "# Part II: Clustering on Geographic attributes\n",
    "\n",
    "Now, let's separately handle looking at dimensionality reduction for the geographic attributes. Our goal would be to see if we can extract some `pca` components that reduce some of the geography."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4f6c3",
   "metadata": {},
   "source": [
    "### II.I: Create `geo_df`\n",
    "\n",
    "Let's get a DataFrame stood up with one record for each listing, and all of the geographic data we have on the listing on the area the listing is located.\n",
    "\n",
    "* Extract all geographic data out of `airbnb_df` into `airbnb_df_geo`.\n",
    "* Join `airbnb_df_geo` with `county_geo` and `parks_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geo-data in airbnb_df.\n",
    "airbnb_df_geo = airbnb_df[['listing_id','country','state','city','county','postcode']]\n",
    "\n",
    "# Drop FIPS code.\n",
    "county_geo = county_df.drop(columns=['FIPS'])\n",
    "\n",
    "# A quick cleaner function to help join the datasets.\n",
    "def remove_county(x):\n",
    "    try:\n",
    "        return x.replace(' County','')\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "\n",
    "# Clean county data by removing ' County' from name.\n",
    "airbnb_df_geo['county'] = airbnb_df_geo['county'].apply(remove_county)\n",
    "\n",
    "# Rename county_name to county. This approach is a little lazy.\n",
    "county_geo['county'] = county_geo['county_name']\n",
    "\n",
    "# Merge data.\n",
    "geo_df = airbnb_df_geo.merge(county_geo,how='left',on=['state','county'])\n",
    "geo_df = geo_df.merge(parks_df,how='left',on='state')\n",
    "\n",
    "# Look at first five records.\n",
    "geo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d896df0",
   "metadata": {},
   "source": [
    "### II.II Prepare data for PCA.\n",
    "\n",
    "Similar to what we did with `airbnb_df`, let's prepare `geo_df` for a Principal Components Analysis.\n",
    "\n",
    "* Save numerical values of interest in X.\n",
    "* Normalize data with `StandardScaler().\n",
    "* Impute missing data with median.\n",
    "* Replace any `inf` values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab939a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = geo_df['listing_id']\n",
    "\n",
    "X = geo_df[['rural-urban-continuum-code','perc-college-educated-2016-2020','perc_unemployed_2021',\n",
    "           'median_hh_income','perc_state_median_hh_income','perc_people_poverty_2020',\n",
    "           'perc_children_poverty_2020','count_state_park','count_national_park','percent_park']]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ce4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = StandardScaler().fit(X).transform(X)\n",
    "\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "X_normalized = imp_median.fit_transform(X_normalized)\n",
    "\n",
    "X_normalized[X_normalized == -np.inf] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1bab9f",
   "metadata": {},
   "source": [
    "### II.III PCA with Geographic Data.\n",
    "\n",
    "Let's see if we have better luck with this smaller dataset finding good principal components.\n",
    "\n",
    "* Create PCA model `pca` with 10 components fit on `X_normalized`.\n",
    "* Visualize the first 2 dimensions.\n",
    "* Visualize the relationship between each component and the variables.\n",
    "* Visualize the explained variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 10, random_state=0).fit(X_normalized)\n",
    "pca_values = PCA(n_components = 10).fit_transform(X_normalized)\n",
    "\n",
    "plt.scatter(pca_values[:,0], pca_values[:,1])\n",
    "plt.title('First two dimensions: PCA')\n",
    "plt.xlabel('2nd dimension')\n",
    "plt.ylabel('1st dimension')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(pca, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'explained var ratio':pca.explained_variance_ratio_,\n",
    "                   'PC':['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']})\n",
    "sns.barplot(x = 'PC',y = \"explained var ratio\", data = df, color=\"c\");\n",
    "np.cumsum(df['explained var ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58510973",
   "metadata": {},
   "source": [
    "### Analysis: Principal components.\n",
    "\n",
    "`pca` did a fairly good job here in dimensionality reduction. The first 3 components explain 86.4% of the variance. Additionally, the factors that relate to each component tell a human interpretable story about what is captured in each component:\n",
    "\n",
    "* **1st component (43.19% of variance):** Positive correlation with `perc_people_in_poverty_2020`, `perc_children_in_poverty_2020`, and `rural_urban_continuum_code` (higher score here implies a more rural area); negative correlation with `perc-college-educated-2016-2022 `, `median_hh_income`, and `perc_median_state_household_income`. Listings with a high score on the first component are more rural, less wealthy, and less educated.\n",
    "* **2nd component (34.19% of variance:** Positive correlation with `count_state_park`, `count_national_park`, `percent_park`, and `perc_unemployed_2021`; slight negative correlation with `rural_urban_continuum_code` and `perc_college_educated_2016-2022`. Listings with a high score on the second component are slightly more urban, slightly less educated and employed, and (most importantly) in states with a large parks presence. These are likely cities and counties with sub-urban communities near the state's median income.\n",
    "* **3rd component (9.02%):** Positive correlation with `perc_unemployed_2021`; negative correlation with `rural-urban-continuum-code`, `count_national_park`, `perc_parks`. Listings with a high score on the third component are more urban, have higher unemployment, and have less national parks presence in the state. This likely captures urban communities in states with less parks (such as large cities in New Mexico, Nevada, or Idaho).\n",
    "\n",
    "We don't necessarily need to go through the exercise of using `umap` to try and find clusters. These reduced dimensions (in combination with the `umap` clusters derived from the listings data) should sufficiently enrich the data to try and build an interesting supervised learning model on predicting price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df['pca_1st_component'] = pca_values[:,0] \n",
    "airbnb_df['pca_2nd_component'] = pca_values[:,1]\n",
    "airbnb_df['pca_3rd_component'] = pca_values[:,2]\n",
    "airbnb_df['pca_4th_component'] = pca_values[:,3]\n",
    "airbnb_df['pca_5th_component'] = pca_values[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac90af4",
   "metadata": {},
   "source": [
    "# III. Save Outputs.\n",
    "\n",
    "Let's save the outputs of this analysis to carry forward to the Supervised Machine Learning notebook (3.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5210165",
   "metadata": {},
   "source": [
    "### III.I Save data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5a99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "airbnb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df.to_csv('UNSUPERVISED_LEARNING airbnb_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee15ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
