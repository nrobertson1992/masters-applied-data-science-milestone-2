{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509a6f8e",
   "metadata": {},
   "source": [
    "# Webscrape AirBnB Search\n",
    "\n",
    "AirBnB is a popular short-term rental listing site. It can be used to find 100,000's of listings a user can rent from home owners.\n",
    "\n",
    "This first notebook, part of a larger project, is focused on scraping data from the search page of AirBnB. In broad brush strokes, the notebook accomplishes the following.\n",
    "\n",
    "* Create 40,000~ unique links that take box-by-box map searches of the western United states, from California to the Colorado-Kansas border.\n",
    "* Request the first page of search results. If there are multiple pages, click through each page of the results.\n",
    "* On each page, get the listings on the page.\n",
    "* Format the data into a structured DataFrame.\n",
    "\n",
    "This won't handle all of the data cleaning -- right now, we're just primarily concerned with getting the data.\n",
    "\n",
    "_(An example screenshot of the search page that is scraped.)_\n",
    "\n",
    "\n",
    "![Image of AirBnB search](https://lh4.googleusercontent.com/7FeLyHckAoGePixHRazECphf4gNZbem5huowTH-VpFTiQfcaEos8_1aKWQOZQ0Zq3j8=w2400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcccd57",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a0ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webscraping.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from requests_html import HTMLSession\n",
    "import json\n",
    "\n",
    "# For standard data manipulation.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# For progress tracking.\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Makes it easier to see all the columns in wide dataframes!\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b82c021",
   "metadata": {},
   "source": [
    "# Set headers for webscraping\n",
    "\n",
    "This will set the headers we'll use for scraping. This will help our requests look more \"human\" to AirBnB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0afe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cookie': 'OptanonAlertBoxClosed=NR; OptanonAlertBoxClosed=NR; bev=1671467614_NDg3YWI1OWViODE0; country=US; everest_cookie=1671467934.o8UVAqq76D3QPRXxo5SV.2TXJ26qIr6wRp2aGPSqpNu0hmcaL7JhFzmDBWbnzmDw; _csrf_token=V4%24.airbnb.com%24Dwa4m1DnRds%24l_ThNsfxVtYzJnV3gf7vehcjPJusOG_QBcYxaP2lZOM%3D; flags=0; _gcl_au=1.1.1979572412.1671467936; _ga=GA1.1.923589191.1671467936; tzo=-420; frmfctr=wide; ak_bmsc=2BF62A04339B5A8040F6BE42D7397A3E~000000000000000000000000000000~YAAQjuXEF4UQhgeFAQAA27JjLRJCqxbQs87wSwEdHRYX0G6/TDKZTovu4o6yCZ81IqZ6MhWckkFsgOG1eRvCyGcwYcGwZT6Hj0lKPA7+LECJW/p7uLVBPkdj9vDMc+M3APPxtp3dA4N7Xiz9k2CsK1cScWB15o99WkNT8TblkPRJMP+//tibMWG2xoEjeEV14znNbpxu/a19eOVXAFS6pvZFEKc1PvKaN6qVTNMeaHczy8vtIlUIhOHm8n0/neIB5WjhQewZ2bsI4Jk5FAyuugtHiXFiRJU7poWmTC1z3vQV5BNZly4umCoBfxL+4/1Gwtvg+5jzWIgEsQIOlNvhj6IhTWNdgIiUubyhm6SwXUHdD2r80FCzSqqkgs/F95M/C2eTuiMmpV414Qo=; jitney_client_session_id=d94ad55f-160c-4b75-aa19-6b94073c92fc; jitney_client_session_created_at=1671505980; _user_attributes=%7B%22curr%22%3A%22USD%22%2C%22guest_exchange%22%3A1.0%2C%22device_profiling_session_id%22%3A%221671467934--953d4ec0c3e885ffcfcecdcb%22%2C%22giftcard_profiling_session_id%22%3A%221671503786--bedf36f5fd7bc1727463feba%22%2C%22reservation_profiling_session_id%22%3A%221671503786--104feb525ef2701da84b3359%22%7D; jitney_client_session_updated_at=1671505982; _ga_2P6Q8PGG16=GS1.1.1671505983.4.1.1671505983.0.0.0; _uetsid=a14c3a907fbb11ed80888db3ffb5b814; _uetvid=a14c54c07fbb11edbfa35d16156e96a6; previousTab=%7B%22id%22%3A%22c5e39694-2720-411d-bbb4-5880ad1455b6%22%2C%22url%22%3A%22https%3A%2F%2Fwww.airbnb.com%2Fs%2FUnited-States%2Fhomes%22%7D; bm_sv=67E8D97D583DC2BA73262B2A33B8BD6A~YAAQjuXEF3gehweFAQAAzlqFLRKkxd6iaP1xq0hHbfO3CsyIIQifNN1kg5O8tYdobGiFUFkAU0ek5Tg67B28nu2btfk+1yHhT6TE1jibTRrmUjxmCnS6gZ9Sclthu91yIEEePEd32A66YE2G6ofZjSQCTS8TsZU9O8pKFQ7Nid5vVQ6DlyEPLX5yEYMLK94A1kBNulzVg+KjaWf2UN8pSkr66hIUxNn5Qq3fQtl8kCxJeSb6MGW977OC++DGDRXKqg==~1; cfrmfctr=MOBILE; cbkp=2',\n",
    "    'device-memory': '8',\n",
    "    'dpr': '2',\n",
    "    'ect': '4g',\n",
    "    'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\", \"Google Chrome\";v=\"108\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'empty',\n",
    "    'sec-fetch-mode': 'no-cors',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'viewport-width': '895'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d77c43",
   "metadata": {},
   "source": [
    "# Create the helper functions required to run the scraper\n",
    "\n",
    "Let's set up a few functions we'll use to breathe life into this webscraper.\n",
    "\n",
    "\n",
    "### def `reset_dataframe`:\n",
    "**Args:** \n",
    "* None.\n",
    "\n",
    "**Returns:**\n",
    "* Empty DataFrame with the shape and headers of the data we intend to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51a1930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_dataframe():\n",
    "\n",
    "    # Create the dataframe.\n",
    "    listings_dataframe = pd.DataFrame(\n",
    "            {'listing_id':       [],\n",
    "             'listing_url':      [],\n",
    "             'is_superhost':     [],\n",
    "             'rating':           [],\n",
    "             'n_reviews':        [],\n",
    "             'listing_city':     [],\n",
    "             'listing_title':    [],\n",
    "             'n_pictures':       [],\n",
    "             'room_type':        [],\n",
    "             'latitude':         [],\n",
    "             'longitude':        [],\n",
    "             'beds':             [],\n",
    "             'price':            [],\n",
    "             'discounted_price': [],\n",
    "             'original_price':   [],\n",
    "             'price_qualifier':  [],\n",
    "             'image_1':          [],\n",
    "             'image_2':          [],\n",
    "             'image_3':          [],\n",
    "             'image_4':          [],\n",
    "             'image_5':          []\n",
    "            })\n",
    "    \n",
    "    return listings_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc7166",
   "metadata": {},
   "source": [
    "### def `first_page`:\n",
    "**Args:** \n",
    "* url: A URL on a search page that will be scraped using the `request` Python package.\n",
    "* headers: The pre-set headers used to make the requests look more human.\n",
    "\n",
    "**Returns:**\n",
    "* Either a `BeautifulSoup` soup object of the webpage, or a None object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c243a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page(url, headers=headers):\n",
    "        \n",
    "    try:\n",
    "        r = requests.get(url, headers=headers)\n",
    "        text = r.text\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        return soup\n",
    "    except:\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers)\n",
    "            text = r.text\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            return soup\n",
    "        except:\n",
    "            try:\n",
    "                r = requests.get(url, headers=headers)\n",
    "                text = r.text\n",
    "                soup = BeautifulSoup(text, 'html.parser')\n",
    "                return soup\n",
    "            except:\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a0043b",
   "metadata": {},
   "source": [
    "### def `extract_listing_info`:\n",
    "**Args:** \n",
    "* obj: a JSON data structure object that is parsed out of a `BeautifulSoup` soup object of a webpage.\n",
    "* df: a DataFrame with all of the listings scraped to date. For the first listing, it is the empty DataFrame generated by `reset_dataframe()`. For the second listing and onwards, it is a DataFrame of all the prior data scraped.\n",
    "\n",
    "**Returns:**\n",
    "* a DataFrame of all the listings scraped up to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea6d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_listing_info(obj, df):\n",
    "    # Extract all the information about the listing from the search page.\n",
    "    try:\n",
    "        try:\n",
    "            listing_id       = obj['listing']['id']\n",
    "        except:\n",
    "            listing_id = None\n",
    "\n",
    "        try:\n",
    "            listing_url      = 'https://www.airbnb.com/rooms/' + obj['listing']['id']\n",
    "        except:\n",
    "            listing_url = None\n",
    "        try:\n",
    "            is_superhost     = obj['listing']['formattedBadges'][0]['text']\n",
    "        except:\n",
    "            is_superhost = None\n",
    "        try:\n",
    "            rating           = obj['listing']['avgRatingLocalized'].split(' ')[0]\n",
    "        except:\n",
    "            rating = None\n",
    "        try:   \n",
    "            n_reviews        = obj['listing']['avgRatingLocalized'].split(' ')[1][1:-1]\n",
    "        except:\n",
    "            n_reviews = None\n",
    "        try:\n",
    "            listing_city     = obj['listing']['city']\n",
    "        except:\n",
    "            listing_city = None\n",
    "        try:\n",
    "            listing_title    = obj['listing']['title']\n",
    "        except:\n",
    "            listing_title = None\n",
    "        try:\n",
    "            n_pictures       = obj['listing']['contextualPicturesCount']\n",
    "        except:\n",
    "            n_pictures = None\n",
    "        try:\n",
    "            room_type        = obj['listing']['roomTypeCategory']\n",
    "        except:\n",
    "            room_type = None\n",
    "        try:\n",
    "            latitude         = obj['listing']['coordinate']['latitude']\n",
    "        except:\n",
    "            latitude = None\n",
    "        try:\n",
    "            longitude        = obj['listing']['coordinate']['longitude']\n",
    "        except:\n",
    "            longitude = None\n",
    "        try:\n",
    "            beds             = obj['listing']['structuredContent']['primaryLine'][0]['body']\n",
    "        except:\n",
    "            beds = None\n",
    "        try:\n",
    "            price            = obj['pricingQuote']['structuredStayDisplayPrice']['primaryLine']['price']\n",
    "        except:\n",
    "            price = None\n",
    "        try:\n",
    "            discounted_price = obj['pricingQuote']['structuredStayDisplayPrice']['primaryLine']['discountedPrice']\n",
    "        except:\n",
    "            discounted_price = None\n",
    "        try:\n",
    "            original_price   = obj['pricingQuote']['structuredStayDisplayPrice']['primaryLine']['originalPrice']\n",
    "        except:\n",
    "            original_price = None\n",
    "        try:\n",
    "            price_qualifier  = obj['pricingQuote']['structuredStayDisplayPrice']['primaryLine']['qualifier']\n",
    "        except:\n",
    "            price_qualifier = None\n",
    "        try:\n",
    "            image_1          = obj['listing']['contextualPictures'][0]['picture']\n",
    "        except:\n",
    "            listing_id = None\n",
    "        try:\n",
    "            image_2          = obj['listing']['contextualPictures'][1]['picture']\n",
    "        except:\n",
    "            image_2 = None\n",
    "        try:\n",
    "            image_3          = obj['listing']['contextualPictures'][2]['picture']\n",
    "        except:\n",
    "            image_3 = None\n",
    "        try:\n",
    "            image_4          = obj['listing']['contextualPictures'][3]['picture']\n",
    "        except:\n",
    "            image_4 = None\n",
    "        try:\n",
    "            image_5          = obj['listing']['contextualPictures'][4]['picture']\n",
    "        except:\n",
    "            image_5 = None\n",
    "        \n",
    "        row = pd.DataFrame(\n",
    "            {'listing_id':       [listing_id],\n",
    "             'listing_url':      [listing_url],\n",
    "             'is_superhost':     [is_superhost],\n",
    "             'rating':           [rating],\n",
    "             'n_reviews':        [n_reviews],\n",
    "             'listing_city':     [listing_city],\n",
    "             'listing_title':    [listing_title],\n",
    "             'n_pictures':       [n_pictures],\n",
    "             'room_type':        [room_type],\n",
    "             'latitude':         [latitude],\n",
    "             'longitude':        [longitude],\n",
    "             'beds':             [beds],\n",
    "             'price':            [price],\n",
    "             'discounted_price': [discounted_price],\n",
    "             'original_price':   [original_price],\n",
    "             'price_qualifier':  [price_qualifier],\n",
    "             'image_1':          [image_1],\n",
    "             'image_2':          [image_2],\n",
    "             'image_3':          [image_3],\n",
    "             'image_4':          [image_4],\n",
    "             'image_5':          [image_5]\n",
    "            }\n",
    "        )\n",
    "        # Return dataframe, with new row of data appended.\n",
    "        return pd.concat([df, row], ignore_index=True)\n",
    "    except:\n",
    "        # Return the dataframe in its current state\n",
    "        #print('error: data didn\\'t scrape')\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6486f35",
   "metadata": {},
   "source": [
    "### def `get_next_page`:\n",
    "**Args:** \n",
    "* obj: a JSON data structure object that is parsed out of a `BeautifulSoup` soup object of a webpage.\n",
    "* url: The url of the current page of search results.\n",
    "* headers: The pre-set headers used to make the requests look more human.\n",
    "\n",
    "**Returns:**\n",
    "* A `BeautifulSoup` soup object of the _next_ page of search results.\n",
    "* The url of the _next_ page of search results' url, which will be fed into `get_next_page()` again to see if there is another page of search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10bec947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page(obj, url, headers=headers):\n",
    "\n",
    "    try:\n",
    "        # Get next page link if it is there.\n",
    "        next_page = obj['niobeMinimalClientData'][0][1]['data']\n",
    "        next_page = next_page['presentation']['explore']['sections']\n",
    "        next_page = next_page['sectionIndependentData']['staysSearch']['paginationInfo']['nextPageCursor']\n",
    "        \n",
    "        # If there isn't already a next page cursor in the url:\n",
    "        if 'cursor=' not in str(url):\n",
    "            # add the url param and next page cursor.\n",
    "            url = url + '&pagination_search=true&cursor=' + next_page\n",
    "        # If there is a next page cursor in the url.\n",
    "        elif 'cursor=' in str(url):\n",
    "            # Remove the existing cursor, add the new one.\n",
    "            #url = '='.join(url.split('=')[0:-2]) + '=' + next_page\n",
    "            url = url.split('&pagination_search=true&cursor=')[0] + '&pagination_search=true&cursor=' + next_page\n",
    "        else:\n",
    "            # Just make the url None. It will fail, and the function will move on.\n",
    "            url = None\n",
    "        #print('URL:', url)\n",
    "        r = requests.get(url, headers=headers)                               # Request the HTML.\n",
    "        text = r.text                                                        # Extract the text.\n",
    "        soup = BeautifulSoup(text, 'html.parser')                            # Soupify text.\n",
    "        return soup, {'url': url}\n",
    "        #print('Found next page.')\n",
    "    except Exception as e:\n",
    "        #print('No next page found.')\n",
    "        #print(e)\n",
    "        return None, None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bf620",
   "metadata": {},
   "source": [
    "### def `generate_mapviews`:\n",
    "**Args:** \n",
    "* None.\n",
    "\n",
    "**Returns:**\n",
    "* A list of dictionaries where the 0th value is a url, the 1st value is the estimated latitude of the url, and 2nd value is the estimate value of the url. The list of urls should create a thousands of mapview urls that sum to the size of the box specified by `north_lat`, `south_lat`, `east_lng`, and `west_lng`. Depending on the size of the box, and the size of each incremental snapshot, this list could be in the tens of thousands up to the hundreds of thousands.\n",
    "\n",
    "\n",
    "_Image demonstrating how mapbox filters each request link for search._\n",
    "![Image demonstrating mapbox](https://lh6.googleusercontent.com/irSKqaHTVG0_28c8ma6b-9gcTkVaNOH4Nvg6FClmYnCkDmgTn96BH9FOJhHb1vuc35s=w2400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb9e1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output of URLs:\n",
      "[{'url': 'https://www.airbnb.com/s/United-States/homes?search_type=user_map_move&ne_lat=31.433363318718605&ne_lng=-124.64655654881464&sw_lat=31.333363318718604&sw_lng=-124.74655654881464&tab_id=home_tab', 'est_lat': 31.383363318718605, 'est_lng': -124.69655654881464}, {'url': 'https://www.airbnb.com/s/United-States/homes?search_type=user_map_move&ne_lat=31.533363318718607&ne_lng=-124.64655654881464&sw_lat=31.433363318718605&sw_lng=-124.74655654881464&tab_id=home_tab', 'est_lat': 31.483363318718606, 'est_lng': -124.69655654881464}]\n",
      "# of URLs 40179\n"
     ]
    }
   ],
   "source": [
    "def generate_mapviews():\n",
    "    # The West: Washington state, Oregon, California, Idaho, Nevada, Utah, \n",
    "    # Arizona, Montana, Wyoming, Colorado, New Mexico.\n",
    "    north_lat =   48.9995662408735\n",
    "    south_lat =   31.333363318718604\n",
    "    east_lng  = -102.05125274620897\n",
    "    west_lng  = -124.74655654881464\n",
    "    \n",
    "    coords = []\n",
    "    \n",
    "    # incrementing between western and easternmost longitude, in .01 increments:\n",
    "    for lng in np.arange(west_lng, east_lng,.1):      \n",
    "        # incrementing between southern and northernmost latitude, in .01 increments:\n",
    "        for lat in np.arange(south_lat, north_lat,.1):\n",
    "            # Create a dictionary to define the \n",
    "            coord = {\n",
    "                'ne_lat':lat+.1,\n",
    "                'ne_lng':lng+.1,\n",
    "                'sw_lat':lat,\n",
    "                'sw_lng':lng\n",
    "            }\n",
    "            \n",
    "            coords.append(coord)\n",
    "            \n",
    "    def gen_links(coord):\n",
    "        \n",
    "        return ('https://www.airbnb.com/s/',\n",
    "                'United-States/homes?',\n",
    "                'search_type=user_map_move',\n",
    "                '&ne_lat={}'.format(coord['ne_lat']),\n",
    "                '&ne_lng={}'.format(coord['ne_lng']),\n",
    "                '&sw_lat={}'.format(coord['sw_lat']),\n",
    "                '&sw_lng={}'.format(coord['sw_lng']),\n",
    "                '&tab_id=home_tab')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Store the URL, as long as the estimated lat/long location -- average of the northern/southern most lat,\n",
    "    # and the eastern/western most long in the view.\n",
    "    coord_data = []\n",
    "    \n",
    "    for coord in coords:\n",
    "        dict_ = {\n",
    "            'url': ''.join(gen_links(coord)),\n",
    "            'est_lat':(coord['ne_lat']+coord['sw_lat'])/2,\n",
    "            'est_lng':(coord['ne_lng']+coord['sw_lng'])/2\n",
    "        }\n",
    "        \n",
    "        coord_data.append(dict_)\n",
    "    \n",
    "    return coord_data\n",
    "\n",
    "urls = generate_mapviews()\n",
    "\n",
    "print('Sample output of URLs:')\n",
    "print(urls[0:2])\n",
    "print('# of URLs',len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2de1d5",
   "metadata": {},
   "source": [
    "### def `process_listings`:\n",
    "**Args:** \n",
    "* url: A URL on a search page that will be scraped using the request Python package.\n",
    "* listings_dataframe: a DataFrame with all of the listings scraped to date. For the first listing, it is the empty DataFrame generated by `reset_dataframe()`. For the second listing and onwards, it is a DataFrame of all the prior data scraped.\n",
    "* url_count: A count of the number of urls that have been processed.\n",
    "\n",
    "**Returns:**\n",
    "* A DataFrame with the results of scraping one url. The url is the starting point of `first_page()`, and then each listing object on the page is passed to `extract_listing_info`. If there is more than one page of search results, `get_next_page` will use the url passed to `first_page()` to get to the next page, and then pass it to `extract_listing_info`. Once there are no more pages of search results, the function will exit and return the DataFrame of all the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9479f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the dataframe.\n",
    "listings_dataframe = reset_dataframe()\n",
    "\n",
    "# Keeping track of the number of URLs that have been worked through.\n",
    "url_count = 0\n",
    "\n",
    "def process_listings(url,listings_dataframe=listings_dataframe,url_count=url_count):\n",
    "\n",
    "    frame_list = []\n",
    "    \n",
    "    # Feed the url through to get the soup of the first page.\n",
    "    soup = first_page(url['url'])\n",
    "    # TODO: create printing log to show pagination to make sure it is working.\n",
    "    page_count = 1\n",
    "    # Until the scraper cannot find another page to scrape.\n",
    "    while soup != None:\n",
    "        \n",
    "        #print('Page Count:', page_count)\n",
    "\n",
    "        # Get all of the individual listings on the page.\n",
    "        \n",
    "        try:\n",
    "            string = str(soup.html.find_all('script')[-1])\n",
    "            string_filtered = string[84:-9]\n",
    "            json_obj = json.loads(string_filtered)\n",
    "\n",
    "\n",
    "            json_obj = json_obj['niobeMinimalClientData'][0][1]['data']['presentation']\n",
    "            json_obj = json_obj['explore']['sections']['sectionIndependentData']['staysSearch']['searchResults']\n",
    "\n",
    "            if 'splitStaysListings' not in json_obj:\n",
    "\n",
    "                objs = [obj for obj in json_obj]\n",
    "\n",
    "                # for each listing.\n",
    "                for obj in objs:\n",
    "\n",
    "                    # Append the scraped data to page. Note that we feed\n",
    "                    # the lat/long from the url.\n",
    "                    frame = extract_listing_info(\n",
    "                                    obj=obj,\n",
    "                                    df=listings_dataframe\n",
    "                        )\n",
    "\n",
    "                    #if frame != None:\n",
    "                    frame_list.append(frame)\n",
    "\n",
    "                # Sleep for 1-5 seconds. Just in case someone is monitoring\n",
    "                # my events on AirBnB's side, I want to give at least *some*\n",
    "                # illusion that this is a human being and not a scraper.\n",
    "                time.sleep(random.randint(1,5))\n",
    "\n",
    "            # Attempt to go to next page. If no next page is found, returns\n",
    "            # None and breaks the loop. This completes the loop for one url,\n",
    "            # and the scraper moves onto the next url.\n",
    "            page_count += 1\n",
    "            json_obj = json.loads(string_filtered)\n",
    "            soup, url = get_next_page(obj=json_obj, url=url['url'])\n",
    "\n",
    "            # Tick another url completed.\n",
    "            url_count += 1\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    try: \n",
    "        return pd.concat(frame_list, ignore_index=True)\n",
    "    except:\n",
    "        return reset_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e37327c",
   "metadata": {},
   "source": [
    "# Execute webscraping.\n",
    "\n",
    "_Depending on your wifi connection, this job might get interrupted! There is an affordance in this to read the scraped results in as `extract 1`, and append that with the job when it is started back up._\n",
    "\n",
    "\n",
    "**If True:**\n",
    "(This takes approximately 7-10 days to run, depending on interruptions\n",
    "* Set `url_count` to `0`, and `listing_dataframe` to `reset_dataframe()`.\n",
    "* Create a list of frames, `frames`.\n",
    "* Generate a list of urls, `urls`.\n",
    "* For each url in `urls`, call `process_listings()`, and save the returned results as `frame`.\n",
    "* Append `frame` to `frames`.\n",
    "* Sleep for a random period of time between 1 and 5 seconds to keep rate of requests slow.\n",
    "* For every 100 urls processed, save the results down to `scraped_listings.csv`. This allows us to (1) inspect results and (2) prevent losing all data if the job is interrupted (only up to a hundred will be lost).\n",
    "\n",
    "**If False:**\n",
    "* Just read in the already scraped data, and skip everything above. Allows someone to \"run\" the notebook and see all of the code without setting off the full job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34312dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1884/1884 [3:46:07<00:00,  7.20s/it]  \n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    # count of URLs.\n",
    "    url_count = 0\n",
    "\n",
    "    # Create empty DataFrame.\n",
    "    listings_dataframe = reset_dataframe()\n",
    "\n",
    "    # List of frames.\n",
    "    frames = []\n",
    "\n",
    "    # Generate urls. If job interrupted, use indexing to remove all URLs already processed.\n",
    "    urls = generate_mapviews()[2295+300+1000+1800+100+4200+14100+13800+700:]\n",
    "\n",
    "    # Temp solution: read in all of the already processed listings. Used if job is interrupted.\n",
    "    extract1 = pd.read_csv('extract 1.csv')\n",
    "    extract1.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    # For each urls in the list of generated urls.\n",
    "    for url in tqdm(urls):\n",
    "        # Process all of the listings at that url, including any pagination through the next page of search.\n",
    "        frame = process_listings(url=url)\n",
    "        # Append the listings to the list of results, `frames`.\n",
    "        frames.append(frame)\n",
    "        # Pause the script for a random amount of time, 1 to 5 seconds. Don't want to overwhelm AirBnB with requests!\n",
    "        time.sleep(random.randint(1,5))\n",
    "        # Mark a processed url.\n",
    "        url_count += 1\n",
    "        # For every 100 urls processed:\n",
    "        if url_count % 100 == 0:\n",
    "\n",
    "            # Copy the list of frames.\n",
    "            write_frames = frames.copy()\n",
    "            # Add the extracted listings from prior jobs that were interrupted.\n",
    "            write_frames.append(extract1)\n",
    "            # Write the concatenated list of frames into one csv that is plugged into Tableau for monitoring.\n",
    "            pd.concat(write_frames, ignore_index=True).to_csv('scraped_listings.csv')\n",
    "\n",
    "    # Concatenate all final results into one DataFrame.\n",
    "    write_frames = frames.copy()\n",
    "    write_frames.append(extract1)\n",
    "    listings_dataframe = pd.concat(write_frames, ignore_index=True)\n",
    "if True:\n",
    "    listings_dataframe = pd.read_csv('scraped_listings BACKUP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be94f2",
   "metadata": {},
   "source": [
    "# Deduplicate listings.\n",
    "\n",
    "There might have been a few duplicate listings. Let's fix that!\n",
    "\n",
    "### def `dedupe_data`:\n",
    "**Args:** \n",
    "* listings_dataframe: the scraped listings in DataFrame form.\n",
    "\n",
    "**Returns:**\n",
    "* listings_dataframe: deduplicated copy of the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f71788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedupe listings.\n",
    "def dedupe_data(listings_dataframe):\n",
    "\n",
    "    # Get length of dataset.\n",
    "    predupe = len(listings_dataframe)\n",
    "\n",
    "    # We assume with this method their might be some duplicate scrapes. This method will\n",
    "    # drop the duplicates, and keep the first occurrence.\n",
    "    subset_cols = ['listing_id']\n",
    "    listings_dataframe = listings_dataframe.drop_duplicates(subset=subset_cols, keep='first')\n",
    "    postdupe = len(listings_dataframe)\n",
    "    delta = predupe - postdupe\n",
    "\n",
    "    # Pretty print the results post de-duplication.\n",
    "    print(f'''\n",
    "    After deduplication, the AirBnB listings dataset decreased from {predupe} to {postdupe}, a decrease of\n",
    "    {delta}. Where duplicates were found, the first record was taken.\n",
    "    ''')\n",
    "\n",
    "    # Save the results.\n",
    "    return listings_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87fd7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    After deduplication, the AirBnB listings dataset decreased from 188457 to 186808, a decrease of\n",
      "    1649. Where duplicates were found, the first record was taken.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "listings_dataframe = dedupe_data(listings_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8032d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_dataframe.to_csv('scraped_listings BACKUP.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
